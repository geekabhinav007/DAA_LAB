# Questions of 2 and 4 Marks

## Define Time Complexity of an Algorithm

```text
Time complexity is a measure of how the running time of an algorithm increases as the size of the input to the algorithm increases. It is a way to analyze the efficiency of an algorithm in terms of the resources (primarily time).

The time complexity of an algorithm is usually expressed using Big O notation, which provides an upper bound on the growth rate of the algorithm's running time as the input size increases.

The function f(n) which gives the running time and or space in terms of the input size n is called the time complexity of the algorithm.
```

## What is Asymptotic Notation?

```text
Asymptotic notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. It is used to classify algorithms according to how their running time or space requirements grow as the input size grows.

Asymptotic notation is used to:
- Describe the complexity of an algorithm
- Compare the complexity of different algorithms
- Predict the complexity of an algorithm for a given input size
```

<!-- Link -->

1. <https://drive.google.com/drive/folders/1ryd8P1Kd1kW-SiXjf6WYXUhzg1mtEvb0>

## Define Principle of Optimality

```text
The principle of optimality states that if an algorithm A solves a problem in time f(n), then there is no algorithm that solves the same problem in time less than f(n).

The Principle of Optimality is a concept in mathematics and computer science that is used in the design and analysis of dynamic programming algorithms. It states that an optimal solution to a problem can be constructed from optimal solutions to its subproblems, without having to recompute the same subproblems multiple times.

In other words, if we are given a problem that can be broken down into smaller subproblems, and we can find an optimal solution to each subproblem, then we can use those solutions to construct an optimal solution to the original problem. This is true regardless of the order in which we solve the subproblems, as long as we don't solve the same subproblem more than once.

The principle of optimality is a fundamental concept in dynamic programming because it allows us to efficiently solve problems that would be otherwise computationally infeasible. By breaking down a problem into smaller subproblems and solving each subproblem optimally, we can avoid redundant calculations and dramatically reduce the overall running time of our algorithm.

```

## Compute/examine the upper bound and lower bound of the following functions

f(n) = 5n<sup>2</sup> + 2n + 1

> Answer : O(n<sup>2</sup>)

f(n) = 3n<sup>2</sup> + n

> Answer : O(n<sup>2</sup>)

f(n) = n<sup>3</sup> + 3n<sup>2</sup> + 4n + 2

> Answer : O(n<sup>3</sup>)

f(n) = 17n<sup>4</sup> + 3n<sup>3</sup> + 4n + 8

> Answer : O(n<sup>4</sup>)

f(n) = 16n + log(n)

> Answer : O(n)

f(n) = 3 + 4

> Answer : O(1)

f(n) = a + b

> Answer : O(1)

## Compute / Test / Examine the time complexity (Big(O)) of the following code snippets

```c++
for(int i = 0; i<n; i++){
    for(int j = 0; j<i; j++){
            // do something 
    }
}
```

```c++
for(int i = n; i>1; i = i/2){
            // do something 
}
```

```c++
for(int i = 0; i<n ; i++){

}
```

```c++
for(int i = 0; i<=n ; i++){

}
```

```c++
for(int i = 0; i<n ; i++){
    for(int j = 0; j<n; j++){
            // do something 
    }
}
```

```c++
for(int i = 0; i<n ; i++){
    for(int j = 0; j<n; j++){
            for(int k = 0 ; k< n ; k++>){
                // do something 
            }
    }
}
```

## Analysis of Insertion Sort

## Analysis of Binary Search

## Analysis of Merge Sort

## Analysis of Quick Sort

## Analysis of Selection Sort

## Analysis of Max and Min in an Array
